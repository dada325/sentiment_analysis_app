{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e59d9a5-73c4-4a99-945a-8efb17b0e021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/61/57/402c6a522f26e6bdc8d46cce379bf20f40daa0764578510e4e8a3d3ed1a9/safetensors-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de347713-3209-4d25-84e9-d7035ae58f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"all-data.csv\", encoding=\"ISO-8859-1\", header=None)\n",
    "df.columns = ['Sentiment', 'Text']\n",
    "\n",
    "# Basic list of stopwords\n",
    "basic_stopwords = {\n",
    "    'ourselves', 'hers', 'between', 'yourself', 'but', \n",
    "    'again', 'there', 'about', 'once', 'during', 'out', \n",
    "    'very', 'having', 'with', 'they', 'own', 'an', 'be', \n",
    "    'some', 'for', 'do', 'its', 'yours', 'such', 'into', \n",
    "    'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', \n",
    "    'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', \n",
    "    'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', \n",
    "    'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', \n",
    "    'down', 'should', 'our', 'their', 'while', 'above', 'both', \n",
    "    'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', \n",
    "    'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n",
    "    'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "    'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', \n",
    "    'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', \n",
    "    'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', \n",
    "    'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', \n",
    "    'it', 'how', 'further', 'was', 'here', 'than'\n",
    "}\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in basic_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['Processed_Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Sentiment encoding\n",
    "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "df['Encoded_Sentiment'] = df['Sentiment'].replace(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b21dc4-61ea-456d-a3e8-df922f99bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data first\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Processed_Text'], df['Encoded_Sentiment'].values, test_size=0.2\n",
    ")\n",
    "\n",
    "# Tokenize the split data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=256, return_tensors='tf')\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=256, return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1766c0c1-fc86-4310-83bc-77c771f66bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c015ac6f-5454-4e04-a9d7-aa8c7716c4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "485/485 [==============================] - 554s 1s/step - loss: 0.6374 - accuracy: 0.7358 - val_loss: 0.4871 - val_accuracy: 0.8082\n",
      "Epoch 2/3\n",
      "485/485 [==============================] - 546s 1s/step - loss: 0.3595 - accuracy: 0.8627 - val_loss: 0.4916 - val_accuracy: 0.8021\n",
      "Epoch 3/3\n",
      "485/485 [==============================] - 550s 1s/step - loss: 0.2067 - accuracy: 0.9239 - val_loss: 0.5898 - val_accuracy: 0.8052\n",
      "31/31 [==============================] - 26s 853ms/step - loss: 0.5898 - accuracy: 0.8052\n",
      "[0.5897536873817444, 0.8051546216011047]\n"
     ]
    }
   ],
   "source": [
    "# Convert encodings to tf.Tensor\n",
    "def encode_tf_tensors(encodings):\n",
    "    return {\n",
    "        'input_ids': tf.convert_to_tensor(encodings['input_ids'], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor(encodings['attention_mask'], dtype=tf.int32)\n",
    "    }\n",
    "\n",
    "train_data = (encode_tf_tensors(train_encodings), tf.convert_to_tensor(train_labels, dtype=tf.int64))\n",
    "val_data = (encode_tf_tensors(val_encodings), tf.convert_to_tensor(val_labels, dtype=tf.int64))\n",
    "\n",
    "# Load DistilBERT model for TensorFlow\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data[0], train_data[1], validation_data=val_data, epochs=3, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(val_data[0], val_data[1])\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ae4c143-7676-4e06-b872-6c2eff2575d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.txt',\n",
       " './saved_model/added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./saved_model/\")\n",
    "tokenizer.save_pretrained(\"./saved_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471576db-5e82-4b00-87a4-f859d45fc5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
